{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BECCA Classifier\n",
    "by Shannon Hamilton, Shrestha Mohanty, Steve Trush<br>\n",
    "for INFO 256, Fall 2016, taught by Prof. Marti Hearst\n",
    "<pre>\n",
    "Please ensure you run this notebook in a folder with the following files:\n",
    "    emailgraph.py (depends on Stanford NER package - update this file with parameters for your NER installation!)\n",
    "        - also install py2neo\n",
    "    emotion.py (source: Clement Michard (c) 2015 https://github.com/clemtoy/WNAffect)\n",
    "    wnaffect.py (source: Clement Michard (c) 2015 https://github.com/clemtoy/WNAffect)\n",
    " Folders:\n",
    "    hillary-clinton-emails\n",
    "    wordnet-1.6  (see https://wordnet.princeton.edu/wordnet/download/)\n",
    "    wn-domains-3.2 (see http://wndomains.fbk.eu/wnaffect.html)\n",
    "\n",
    "Ensure you have installed the following packages:\n",
    "nltk\\vader_lexicon (run nltk.download())\n",
    "\n",
    "Install Neo4j (https://neo4j.com/download/community-edition/) and run the graph database server.\n",
    "</pre>\n",
    "\n",
    "Much of our strategy for classifying emotional tone <br>\n",
    "leans on the framework described in the following paper: <br>\n",
    "<b>Identifying Emotional Expressions, Intensities and Sentence level\n",
    "Emotion Tags using a Supervised Framework*</b><br>\n",
    "Dipankar Das and Sivaji Bandyopadhyay\n",
    "https://pdfs.semanticscholar.org/02e1/cd141356cd3ea072179f9e9319f28d013061.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports - the basics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "\n",
    "#sklearn... commented out were experiments\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "#from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.multiclass import OneVsOneClassifier\n",
    "#from sklearn.feature_selection import SelectFromModel\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.feature_selection import RFE\n",
    "#from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#pandas\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "#WordNet-Affect\n",
    "from wnaffect import WNAffect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#EmailGraph - written by us!\n",
    "import emailgraph\n",
    "from emailgraph import EmailGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Emails! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read_file is a helper function to get the ',' delimited CSV into a data frame\n",
    "def read_file(filename):\n",
    "    #get the file\n",
    "    df = pd.read_csv(filename, low_memory=False, delimiter = ',', encoding=\"ISO-8859-1\")\n",
    "    #filter the null data\n",
    "    filtered_data = df[\"RawText\"].notnull()\n",
    "    df_narrative = df[filtered_data]\n",
    "    return df_narrative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get_all_tagged_emails (consolidates all tagged emails into on frame)\n",
    "#emails were tagged in 6 bunches to ensure coder reliability\n",
    "def get_all_tagged_emails():\n",
    "    print('Getting Email files...')\n",
    "    df1 = read_file('.\\\\hillary-clinton-emails\\\\tagged-mail\\\\email_out1.csv')\n",
    "    df2 = read_file('.\\\\hillary-clinton-emails\\\\tagged-mail\\\\email_out2.csv')\n",
    "    df3 = read_file('.\\\\hillary-clinton-emails\\\\tagged-mail\\\\email_out3.csv')\n",
    "    df4 = read_file('.\\\\hillary-clinton-emails\\\\tagged-mail\\\\email_out2set2.csv')\n",
    "    df5 = read_file('.\\\\hillary-clinton-emails\\\\tagged-mail\\\\email_out3set2.csv')\n",
    "    df6 = read_file('.\\\\hillary-clinton-emails\\\\tagged-mail\\\\email_out1set2.csv')\n",
    "    \n",
    "    frames = [df1, df2, df3, df4, df5, df6]\n",
    "    total_df = pd.concat(frames)\n",
    "    total_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #randomize the emails\n",
    "    random_index = np.random.permutation(total_df.index)\n",
    "    df_narrative_shuffled = total_df.ix[random_index]\n",
    "    df_narrative_shuffled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"Total tagged emails: \"+str(df_narrative_shuffled.shape[0]))\n",
    "    \n",
    "    #make sure any emotion emails come up as 1\n",
    "    def normalize(x):\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        return 0\n",
    "    df_narrative_shuffled['Label'] = df_narrative_shuffled['Label'].apply(normalize)\n",
    "    \n",
    "    return df_narrative_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train, Dev and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create the data sets (specify the complete frame and percentages of data for train, dev, and test sets)\n",
    "def create_sets(total_df, train_pct, dev_pct, test_pct):\n",
    "    #we want to ensure equal amounts of emotional data due to high imbalance\n",
    "    df_emo = total_df.loc[total_df['Label'] == 1]\n",
    "    rowe , columne  = df_emo.shape\n",
    "    df_neu = total_df.loc[total_df['Label'] == 0]\n",
    "    rown , columnn  = df_neu.shape\n",
    "\n",
    "    #get number of rows per set\n",
    "    train_size_emo = round(rowe*train_pct)\n",
    "    dev_size_emo = round(rowe*dev_pct)\n",
    "    test_size_emo = round(rowe*test_pct)\n",
    "\n",
    "    train_size_neu = round(rown*train_pct)\n",
    "    dev_size_neu = round(rown*dev_pct)\n",
    "    test_size_neu = round(rown*test_pct)\n",
    "\n",
    "    #get training set\n",
    "    df_train_emo = df_emo[0:train_size_emo-1]\n",
    "    df_train_neu = df_neu[0:train_size_neu-1]\n",
    "    df_train = pd.concat([df_train_emo,df_train_neu])\n",
    "    print(\"Size of training set: \"+str(df_train.shape))\n",
    "    \n",
    "    #get dev set\n",
    "    df_dev_emo = df_emo[train_size_emo:(train_size_emo+dev_size_emo)-1].reset_index(drop=True)\n",
    "    df_dev_neu = df_neu[train_size_neu:(train_size_neu+dev_size_neu)-1].reset_index(drop=True)\n",
    "    df_dev = pd.concat([df_dev_emo,df_dev_neu])\n",
    "    print(\"Size of dev set: \"+str(df_dev.shape))\n",
    "\n",
    "    #get test set\n",
    "    df_test_emo = df_emo[dev_size_emo+train_size_emo:]\n",
    "    df_test_neu = df_neu[dev_size_neu+train_size_neu:]\n",
    "    df_test = pd.concat([df_test_emo,df_test_neu])\n",
    "    print(\"Size of test set: \"+str(df_test.shape))\n",
    "    \n",
    "    return df_train, df_dev, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create_datasets gets the emails and separates dataframes by percentage\n",
    "def create_datasets(train_pct,dev_pct,test_pct):\n",
    "    total_df = get_all_tagged_emails()\n",
    "    return create_sets(total_df, train_pct, dev_pct, test_pct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "legal_words = {\"section\",\"fw\",\"re\",\"ops\",\"fyi\",\"doc no\",\"case no\",\"case\",\"usc\",\"foia\",\"u.s.c\",\\\n",
    "               \"report\",\"attachment\",\"attachments\",\"note\",\"amended\", \"ebook\",\"subject\",\"unclassified department of state case\",\"doc\",\\\n",
    "               \"unclassified\",\"original message\",\"project\", \"copyright\", \"pls\", \"pis\",\"state\"}\n",
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = []\n",
    "    processed = \"\"\n",
    "    \n",
    "    #filter out some of the markup lines of text\n",
    "    for line in text.split('\\n'):\n",
    "        s = line.lower()\n",
    "        if s.startswith(\"unclassified u.s. department of state\") or \\\n",
    "            s.startswith(\"release in\"):\n",
    "                 pass\n",
    "        else:\n",
    "            processed = processed + line + '\\n'\n",
    "\n",
    "    #tokenize the sentences        \n",
    "    sents = [s for s in nltk.sent_tokenize(processed)]\n",
    "    for s in sents:\n",
    "        #get word tokens for words that are important (not 'legal words')\n",
    "        tokens = tokens + [word for word in nltk.word_tokenize(s) if word not in legal_words]\n",
    "    filtered_tokens = []\n",
    " \n",
    "    # filter out any tokens containing numbers and ensure at least some letters\n",
    "    for token in tokens:\n",
    "        if not re.search('[0-9]', token):\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "    #return stems\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is from INFO 256 Marti Hearst's POS Tagger exercise\n",
    "def create_data_sets(sentences):\n",
    "    size = int(len(sentences) * 0.9)     #This keeps 10% of the sentences as test data.\n",
    "    train_sents = sentences[:size]\n",
    "    test_sents = sentences[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "#this is from INFO 256 Marti Hearst's POS Tagger exercise\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')       #I changed the default to Proper Noun\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "#train the tagger with some specific sentences (if needed)\n",
    "def train_tagger_with_more():\n",
    "    my_sents = []\n",
    "    tagged_sents = nltk.corpus.nps_chat.tagged_posts() #use the chat corpus for 'informal' speak\n",
    "    return train_tagger(my_sents + tagged_sents)\n",
    "\n",
    "#this is from INFO 256 Marti Hearst's POS Tagger exercise\n",
    "def train_tagger(already_tagged_sents):\n",
    "    train_sents, test_sents = create_data_sets(already_tagged_sents)\n",
    "    ngram_tagger = build_backoff_tagger(train_sents)\n",
    "    print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.858 pos accuracy on test set\n"
     ]
    }
   ],
   "source": [
    "chat_tagger = train_tagger_with_more() #train an informal speech tagger\n",
    "\n",
    "#This is from Clement Michard (c) 2015\n",
    "#https://github.com/clemtoy/WNAffect\n",
    "wna = WNAffect('wordnet-1.6/', 'wn-domains-3.2/') #create a wordnet affect dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here I define a tokenizer, do POS tagging, and to look for emotion words\n",
    "def tokenize_and_emote(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = []\n",
    "    processed = \"\"\n",
    "    for line in text.split('\\n'):\n",
    "        s = line.lower()\n",
    "        #filters the header information in the emails\n",
    "        if s.startswith(\"unclassified u.s. department of state\") or \\\n",
    "            s.startswith(\"release in\") or \\\n",
    "            s.startswith(\"original message\") or \\\n",
    "            s.startswith(\"to:\") or \\\n",
    "            s.startswith(\"from:\") or \\\n",
    "            s.startswith(\"sent:\") or  \\\n",
    "            s.startswith(\"cc:\"):\n",
    "                 pass\n",
    "        else:\n",
    "            processed = processed + line + '\\n'\n",
    "    sents = [s for s in nltk.sent_tokenize(processed)]\n",
    "    for s in sents:\n",
    "        #POS tag the tokens!\n",
    "        tokens = tokens + [word for word in chat_tagger.tag(nltk.word_tokenize(s))] \n",
    "\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    # filter out numbers and then get emotion category from WordNetAffect\n",
    "    for token in tokens:\n",
    "        if not re.search('[0-9]', token[0]):\n",
    "            if re.search('[a-zA-Z]', token[0]):\n",
    "                filtered_tokens.append(wna.get_emotion(token[0].lower(),token[1]))\n",
    "    \n",
    "    emotions = [str(t) for t in filtered_tokens if t is not None]\n",
    "    #print(stems)\n",
    "    return emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove_articles: looks for the start of news articles and strips any text that follows\n",
    "def remove_articles(text):\n",
    "    m = re.search(r\"\\(Reuters\\) \\-|\\(AP\\) \\-|\\(Associated Press\\) \\-|http\\:\\\\\\\\\", text)\n",
    "    if m is not None:\n",
    "        return text[:m.start(0)]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "#how_emo - logarithmic transform of emotional weights\n",
    "def how_emo(x):\n",
    "    if int(x * 100) > 0:\n",
    "        return int(math.log((x * 100)+1, 2)) + 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#simply counts the number of linebreaks in an email\n",
    "def count_lines(text):\n",
    "    return len(text.split('\\n'))\n",
    "\n",
    "#LengthTransformer: Feature is the log_10 of the number of characters in an email\n",
    "class LengthTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        #print(X)\n",
    "        lines = DataFrame(X.apply(lambda x: int(math.log(len(x), 10))))\n",
    "        #print(lines)\n",
    "        return lines\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#count_puncts: returns the number of expressive punctuation and intensifiers in a text\n",
    "def count_puncts(text):\n",
    "    m = re.findall(r\"(?:[\\!])|(?:very)|(?:so much)|(?:\\.\\.\\.)|(?:thx)|(?:thank)\",text.lower())\n",
    "    #m = re.findall(r\"(?:[\\!][\\!])\",text)\n",
    "    if m is not None and len(m) > 0:\n",
    "        return len(m) \n",
    "    else:\n",
    "        return 0    \n",
    "\n",
    "#PuncTransformer: Feature is the number of expressive punctuation and intensifiers per line of text\n",
    "#The scores are adjusted by a logarithmic function\n",
    "#Emails are stripped of news article forwards\n",
    "class PuncTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        lines = DataFrame(X.apply(lambda x: how_emo(count_puncts(remove_articles(x))/\\\n",
    "                                                    count_lines(remove_articles(x)))))\n",
    "        #print(lines)\n",
    "        return lines\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#https://medium.com/@aneesha/quick-social-media-sentiment-analysis-with-vader-da44951e4116#.okgv4siow\n",
    "#https://pypi.python.org/pypi/vaderSentiment\n",
    "#https://github.com/cjhutto/vaderSentiment/issues/5\n",
    "#http://www.nltk.org/howto/sentiment.html\n",
    "class SentimentTransformer(TransformerMixin):\n",
    "    \n",
    "    #assigns sentiment into 4 buckets based on intensity\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    def get_sentiment(self, text):\n",
    "        result = self.sid.polarity_scores(text[0:500]) \n",
    "        #print(result)\n",
    "        if result['neu'] > .9 :\n",
    "            return 0\n",
    "        elif result['neu'] > .8 :\n",
    "            return 1\n",
    "        elif result['neu'] > .7 :\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    \n",
    "    #Feature: a score of 0 to 3 based on the degree of +/- sentiment\n",
    "    def transform(self, X, **transform_params):\n",
    "        lines = DataFrame(X.apply(lambda x: self.get_sentiment(remove_articles(x))))\n",
    "        #print(lines)\n",
    "        return lines\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#EmoTransformer: Feature is the number of 'emotions' in an email per line of text\n",
    "#The scores are adjusted by a logarithmic function\n",
    "#Emails are stripped of news article forwards    \n",
    "class EmoTransformer(TransformerMixin):  \n",
    "    def transform(self, X, **transform_params):       \n",
    "        lines = DataFrame(X.apply(lambda x: \\\n",
    "                how_emo(len(tokenize_and_emote(remove_articles(x)))/count_lines(remove_articles(x)))))\n",
    "        #print('emotions' + str(lines))\n",
    "        return lines\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#basic gridsearch based on:\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n",
    "def perform_gridsearch(clf, df_train, df_dev):\n",
    "    parameters = {\n",
    "                    'vect__max_df': (0.5, 0.6, 0.7, 0.8, 0.9, 1.0),\n",
    "    #                 'clf__penalty': ('l2', 'elasticnet')\n",
    "    #                 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "                   'tfidf__smooth_idf': (True, False),\n",
    "    #                'tfidf__sublinear_idf': (True, False),\n",
    "                    'clf__penalty': ('l1', 'l2', 'elasticnet'),\n",
    "    #                 'clf__dual': (True, False),\n",
    "    #                 'clf__loss': ('hinge','squared_hinge')\n",
    "     }\n",
    "\n",
    "    gs_clf = GridSearchCV(clf, parameters, n_jobs=-1)\n",
    "    gs_clf = gs_clf.fit(df_train[\"RawText\"], df_train.Label)\n",
    "    predicted = gs_clf.predict(df_dev[\"RawText\"])\n",
    "    accuracy_score(df_dev.Label, predicted)\n",
    "    \n",
    "    print(\"Best score: %0.3f\" % gs_clf.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "       print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Code derived from INFO256 notebook: Classification with Scikit-Learn\n",
    "#By John Semerdjian, Andrea Gagliano, and Marti Hearst\n",
    "#Pipeline + FeatureUnion and custom transformers is\n",
    "#built from http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "\n",
    "def train_classifier(df_train, df_dev):\n",
    "    \n",
    "#Comments are other classifiers used\n",
    "#     clf1 = LogisticRegression(random_state=1)\n",
    "#     clf2 = SGDClassifier(loss='log',alpha=1e-4, penalty='l2', n_iter=50, random_state=69)\n",
    "#     clf3 = MultinomialNB()\n",
    "    text_clf = Pipeline([\n",
    "      ('features', FeatureUnion([\n",
    "        ('ngram_tf_idf', Pipeline([\n",
    "            #Bag of words/Bigrams, counts/TFIDF as features\n",
    "            ('vect', CountVectorizer(ngram_range=(1,2), min_df = .2, max_df = .75, tokenizer=tokenize_and_stem, stop_words='english')),\n",
    "            ('tfidf', TfidfTransformer(use_idf=True,norm='l1'))\n",
    "        ])),\n",
    "         ('email_length', LengthTransformer()),\n",
    "         ('jubilant', PuncTransformer()),\n",
    "         ('sentiment', SentimentTransformer()),\n",
    "         ('emotions', EmoTransformer())\n",
    "      ])),\n",
    "#         ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l2\"))),\n",
    "#         ('clf', RandomForestClassifier(class_weight={1:5}))\n",
    "        ('clf', LinearSVC(C=1, random_state=42, penalty='l2', dual=True, tol=1e-5, class_weight='balanced'))\n",
    "#         ('clf', SGDClassifier(loss='hinge', alpha=1e-4, penalty='l2', n_iter=50, random_state=69, class_weight={1:5}))\n",
    "#         ('clf',VotingClassifier(estimators=[('lr', clf1), ('sgdc', clf2), ('lsvc', clf3)], voting='hard'))\n",
    "    ])\n",
    "    text_clf.fit(df_train[\"RawText\"], df_train.Label)\n",
    "    predicted = text_clf.predict(df_dev[\"RawText\"])\n",
    "    #perform_gridsearch(text_clf, df_train, df_dev)\n",
    "    \n",
    "    print(accuracy_score(df_dev.Label, predicted))\n",
    "    print(pd.crosstab(df_dev.Label, predicted, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    print('Yes!')\n",
    "    return text_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried precision/recall adjustments as we were comfortable with more false positives (emails being incorrectly classified as having an emotional tone), but wanted most emotional emails classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://stats.stackexchange.com/questions/140266/classifier-with-adjustable-precision-vs-recall\n",
    "def increase_recall(clf, df_dev):\n",
    "    y_score = clf.decision_function(df_dev[\"RawText\"])\n",
    "    prcsn,rcl,thrshld=precision_recall_curve(df_dev.Label,y_score)\n",
    "    min_prcsn=.25 # here is your precision lower bound e.g. 25%\n",
    "    min_thrshld=min([thrshld[i] for i in range(len(thrshld)) if prcsn[i]>min_prcsn])\n",
    "    y_pred_adjusted=[1 if y_s>min_thrshld else 0 for y_s in y_score]\n",
    "    new_preds = np.array(list([1 if y_pred_adjusted[i] < 1 else predicted[i] for i in range(0,len(predicted))]))\n",
    "    return new_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_error\n",
    "#Originally Coded by Avi Dixit for INFO256\n",
    "#Prints emotional emails if incorrectly categorised by the predictor to be able to fine tune the classifier\n",
    "\n",
    "def print_errors(df_dev, predicted):\n",
    "    devs = df_dev[[\"RawText\",\"Label\"]].values.tolist()\n",
    "    predict = list(predicted)\n",
    "\n",
    "    categories = [\"Neutral\",\"Emotion\"]\n",
    "    ind = 0\n",
    "    for (z,y) in zip(devs, predict):\n",
    "        if ind > 10:\n",
    "            break\n",
    "        if (str(z[1]) == str(y) or (int(z[1]) == 0)):\n",
    "            continue\n",
    "        else:\n",
    "            ind += 1\n",
    "            print(\"Predicted is {}\".format(categories[int(y)]))\n",
    "            print(\"Actual is {}\".format(categories[int(z[1])]))\n",
    "            print(\"Sentence:  {} \\n\". format(z[0]))\n",
    "    #print(\"Total incorrect are {}\".format(ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Classifying the rest of our data (getting emotions too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classify_data, classifies a given dataframe\n",
    "def classify_data(email_classifier, df):\n",
    "    predicted = email_classifier.predict(df[\"RawText\"])\n",
    "    print(accuracy_score(df.Label, predicted))\n",
    "    print(pd.crosstab(df.Label, predicted, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "    #stores predicted labels in a new column\n",
    "    df[\"NewLabel\"] = predicted\n",
    "    data_list = df[\"RawText\"].values.tolist()\n",
    "    \n",
    "    #get the emotion words too\n",
    "    df[\"Emotions\"] = [0] * len(data_list)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    for index in range(0, len(data_list)):\n",
    "         df[\"Emotions\"].loc[index] = \";\".join([w for w in (set(tokenize_and_emote(data_list[index])))])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the Emails in a Graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creates a graph and adds a classified data frame\n",
    "def create_graph(df):\n",
    "    graph = EmailGraph(\"neo4j\",\"becca\")        #the username , password pair\n",
    "    graph.delete()                             #wipes the graph clean\n",
    "    graph.add_new_emails(df.shape[0], df)      #adds the new emails\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together (this function runs the show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Email files...\n",
      "Total tagged emails: 999\n",
      "Size of training set: (797, 12)\n",
      "Size of dev set: (98, 12)\n",
      "Size of test set: (100, 12)\n",
      "0.785714285714\n",
      "Predicted   0   1  All\n",
      "True                  \n",
      "0          59  17   76\n",
      "1           4  18   22\n",
      "All        63  35   98\n",
      "Yes!\n",
      "0.79\n",
      "Predicted   0   1  All\n",
      "True                  \n",
      "0          63  15   78\n",
      "1           6  16   22\n",
      "All        69  31  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Esteban\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the create_graph part once you're ready with Neo4j!\n",
    "def final_algorithm():\n",
    "    train_df, dev_df, test_df = create_datasets(.8,.1,.1)\n",
    "    email_classifier = train_classifier(train_df, dev_df)\n",
    "    result_df = classify_data(email_classifier, test_df)\n",
    "    #create_graph(result_df)\n",
    "    return\n",
    "final_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some sample Neo4J queries..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MATCH p=()-[s:MENTIONS]-(:Email{tone:\"emotional\"}) RETURN p\n",
    "MATCH p=()-[]-(:Email{tone:\"emotional\"}) RETURN p\n",
    "MATCH p=(:Category{name:\"person\"})-[]-()-[]-(:Email{tone:\"emotional\"}) RETURN p\n",
    "MATCH (n)-[]->(e {tone:\"emotional\"}) \n",
    "WHERE any(l IN (n.address) WHERE l=~'.*clintonemail.*')\n",
    "RETURN distinct n, e;\n",
    "\n",
    "MATCH (n)-[]-(i)-[:EMOTED]-(e) \n",
    "WHERE any(l IN (n.address) WHERE l='H')\n",
    "RETURN distinct n,i,e;\n",
    "\n",
    "MATCH (n:User)-[]-(i)-[:EMOTED]-(e) \n",
    "WHERE any(l IN (e.name) WHERE l='anger')\n",
    "RETURN distinct n,i,e;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
